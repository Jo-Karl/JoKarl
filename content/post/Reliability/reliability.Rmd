---
title: What's the deal with omega
author: Johannes Karl
date: '2018-10-07'
slug: omega
link-citations: true
bibliography: [reliability.bib] 
categories:
  - R
tags:
  - R
header:
  caption: ''
  image: ''
---

```{r data, include = F}
library(tidyverse)
setwd(here::here())
dat <- read_csv("static/files/alpha.csv")
```

# The Background

This post is in essence a summary of McNeish's [-@McNeish2018] and Raykov and Marcoulides's [-@Raykov2017] "exchange" on reliability measurement using Cronbach's alpha. I want to add a few additional perspectives to this topic and provide a short How-To in R. 

# What is reliability 


Many students might learn what reliability is, but once they reach research praxis they often have forgotten the details and what remains is the $\alpha > .70$ rule. To address this issue first, this rule was not suggested by Cronbach. The cut-off was suggested by Nunnally [-@Nunnally1994]. Let's see what the original source says:

>"A satisfactory level of reliability depends on how a measure is being used. In the early
stages of predictive or construct validation research, time and energy can be saved using instruments that have only modest reliability, e.g., .70. If significant correlations
are found, corrections for attenuation will estimate how much the correlations will increase when reliabilities of measures are increased. If these corrected values look
promising, it will be worth the time and effort to increase the number of items and reduce
much beyond .80 in basic research is often wasteful of time and money. Measurement
error attenuates correlations very little at that level. Strenuous and unnecessary efforts
at standardization in addition to increasing the number of items might be required to
obtain a reliability of, say, .90. In contrast to the standards used to compare groups, a reliability of .80 may not be nearly high enough in making decisions about individuals. Group research is often concerned with the size of correlations and with mean differences among experimental treatments, for which a reliability of .80 is adequate. However, a great deal hinges on the exact test scores when decisions are made about individuals. If, for example, children with IQs below 70 are to be placed in special classes, it may make a great deal of difference whether a child has an IQ of 65 or 75 on a particular test. When selection standards are quite rigorous, decisions depend on very small score differences, and so it is difficult to accept any measurement error. We have noted that the standard error of measurement is almost one-third as large as the overall standard deviation of test
scores even when the reliability is .90. If important decisions are made with respect to
specific test scores, a reliability of .90 is the bare minimum, and a reliability of .95
should be considered the desirable standard. However, never switch to a less valid
measure simply because it is more reliable." (pp.264)

To boil down what Nunnally said; reliability can never replace validity, the reliability your measure should achieve is dependent on the context of application. The commonly applied rule of .70 is fitting for some contexts. Specifically contexts where one wants to save time and effort. Lance, Butts, and Michels [-@Lance2006] make the important point that this is not a context that applies in most published papers. In most contexts we would want $\alpha$ to be over .80 and over .90 | .95 if we make judgments about individuals based on this test, such as personality questionnaires used for hiring. 

In the discussion on the applicability of $\alpha$ Nunnally makes statements such as:

>"Coefficient a usually provides a good estimate of reliability
because sampling of content is usually the major source of measurement error
for static constructs and also because it is sensitive to the "sampling" of situational
factors as well as item content." (p.252)

But what is the "usual" case which Nunnally assumes. We could assume that it is the case in which the underlying assumption of $\alpha$ are met. Let's have a look at these assumptions to better appreciate Nunnally's usual case. I am paraphrasing in this section from the excellent article by McNeish [-@McNeish2018] and I highly recommend reading it for a more detailed discussion. 

1. The scale is uni-dimensional.
2. Scale items are on a continuous scale and normally distributed.
3. The scale adheres to tau equivalence.
4. The errors of the items do not covary.


Let's step through a normal scale we could find in our data, such as well-being.
Participants answered a scale on flourishing and a scale on satisfaction with life. Because we are interested in overall well-being, we decided to combine the individual items into a single score. 

```{r tau_equivalence}
wll <- c("swls_1", "swls_2", "swls_3", "swls_4", "swls_5", "flourishing_1",
"flourishing_2", "flourishing_3", "flourishing_4",
"flourishing_5", "flourishing_6", "flourishing_7", "flourishing_8")
```


Let's get the theoretical assumpitions out of the way first. Regarding assumption 1, we know that the scale is not uni-dimensional, because it is made up from two distinct scales. We can support this looking at the scree plot below (the paralell analysis would show one factor, but for the sake of argument let's assume the scree is right.)

```{r paralelle, echo = F, message = F}
psych::scree(dat[wll])
normal <- MVN::mvn(dat[wll])
```

So we can see we already violate one assumption of Cronbach's $\alpha$. You can see similar things in many publications that first report the reliability of facets (e.g. mindfulness) and then report the overall reliability. 

Assumption 2 can be split into the requirement that the data is continous, which Likert-type scales are arguably not [for disagreeing stances see: @Norman2010 and @Carifio2007]. For brevities sake I avoid this discussion for now, on the other hand we can assess normality of our data quite easily. As shown below none of our items is normally distributed. 

```{r normality, echo = F}
normal$univariateNormality
```

Assumption 3 is also something we can test quite easily using SEM. We would just compare the fit of the unrestricted model to a model that restricts the loadings of all items on the latent variable to be identical. If we find no substantial decrease in fit we could conclude that we have tau-equivalence. There is also a number of packages that allow you to test for tau-equivalence of your data such as the coefficientalpha package (Which does not seem to be still actively developed and has not the best documentation for functions). In our current case we do not have tau-equivalence.

Assumption 4 is again something that could be easily assessed using a SEM approach. For brevities sake I will omit this, as I think my point is clear. In essence, many commonly used scales violate the assumptions of $\alpha$. And independent of the size of the impact we should strife to use as fitting methods as possible.

# Alternatives

So what are the alternatives to alpha. First I do not think that we need to replace alpha for now Raykov and Marcoulides [-@Raykov2017] make some interesting points on its contiued usage. So instead of replacing it, we could try supplementing it. Alternatives suggested by McNeish [-@McNeish2018] are omega total, and GLB. So how would we compute those reliabilities. We have a number of options from different packages, such as the *psych* package. 


```{r, omega, warning = F, message = F}

psy_omega <- psych::omega(dat[wll], nfactors = 2, poly = F)
psy_omega
```

I personally prefer the *userfriendlyscience* package, which containts the niffty reliability function.

```{r, science, warning = F, message = F}

sci_omega <- userfriendlyscience::reliability(dat[wll])
```

#References